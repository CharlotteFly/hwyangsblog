---
title: 统计学习方法
date: 2017-01-07 09:35:24
tags:
---

##  统计学习
>  如果一个系统能够通过执行某个过程改进它的性能，这就是学习。

###  统计学习的前提
统计学习关于数据的基本假设是同类数据具有一定的统计规律性，这就是统计学习的前提。

###  统计学习方法组成
统计计算由监督学习，非监督学习，半监督学习和强化学习等组成。

### 监督学习概括
从给定的、有限的、用于学习的训练数据集合出发，假设数据是**独立同分布**产生的；并且假设要学习模型属于某个函数集合，称为假设空间；应用某个评价准则，从假设空间中选取一个最优的模型，使它对已知训练数据及未知测试数据在给定的评价准则下有最优的预测；最优模型的选取由算法实现。

###  统计学习方法的三要素
+  模型
+  策略
+  算法

### 实现统计学习方法的步骤
1.  得到一个有限的训练数据集合
2.  确定包含所有可能模型的假设空间，即学习模型的集合
3.  确定模型选择的准则，即学习的策略
4.  实现求解最优模型的算法，取出学习的算法
5.  通过学习方法选择最做强模型
6.  利用学习的最优模型对新数据进行预测和分析

##  基本概念
###  输入空间
输入所有可能的空间

###  输出空间
输出所有可能的空间

###  特征空间
每个具体的输入是一个实例，通常由特征向量表示。这时所有特征向量存在的空间称为特征空间。

###  样本
输入输出对又称为样本或样本点。

###  问题的分类
*  回归问题
>  输入变量与输出变量均为连续变量的预测问题称为回归问题。

*  分类问题  
>  输出变量为有限个离散变量的预测问题称为分类问题。

*  标注问题  
>  输入变量与输出变量均为变量序列的预测问题称为标注问题。

###  联合概率分布
监督学习假设输入与输出的随机变量X与Y遵循联合概率分布。训练数据与测试数据被看作是依联合概率分布P(X,Y)独立同分布产生的。

### 假设空间
模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间。
>  监督学习的模型可以是概率模型或非概率模型，由条件概率分布P(Y|X)或决策函数Y=f(X)表示，随具体学习方法而定。


----------


##  统计学习的三要素
>  方法=模型+策略+算法

###  模型

###  策略
>  损失函数:度量一次预测的好坏
>  风险函数：度量平均意义下模型预测的好坏

####  常用的损失函数
*  0-1损失函数
$$
L(Y,f(X))=
\begin{cases}
1,Y \neq f(X)\\
0,Y = f(X)\\
\end{cases}
$$
*  平方损失函数
$$
L(Y,f(X))=(Y-f(X))^2
$$
*  绝对损失函数
$$
L(Y,f(X))=|Y-f(X)|
$$
*  对数损失函数或对数似然损失函数
$$
L(Y,f(X))=-logP(Y|X)
$$

###  期望风险，损失函数的期望
$$
R_{exp}(f)=E_p[L(Y,f(X)]=\int_{x*y}L(y,f(x))P(x,y)dxdy
$$
称为**风险函数**或期望损失。

>  学习的目标就是选择期望风险最小的模型。由于联合分布p(X,Y)是未知的，$R_{exp}f(x)$不能直接计算。实际上，**如果知道联合分布P(X,Y)，可以从联合分布直接求出条件概率分布P(X,Y)，也就不需要学习了。正因为不知道联合概率分布，所以才需要学习。**

一方面根据期望风险最小学习模型要用到联合分布，另一方面联合分布又是未知的，所以监督学习就成为一个病态问题。

###  经验风险
模型f(X)关于训练数据集的平均损失称为经验风险，或经验损失。记作$R_{emp}$：
$$
R_{emp}(f)=\frac{1}{N}\sum_{n=1}^nL(y_i,f(x_i))
$$

>  期望风险$R_{exp}f(x)$是模型关于联合分布的期望损失，经验风险$R_{emp}(f)$是模型关于样本集的平均损失。

根据大数定律，当样本容量N趋于无穷时，经验风险$R_{emp}(f)$趋于期望风险$R_{exp}(f)$。所以一个很自然的想法是用经验风险估计期望风险。但是，由于现实中训练样本数目有限，甚至很小，所以用经验风险估计期望风险常常并不理想，要对经验风险进行一定的矫正。这就关系到监督学习的两个基本策略：经验风险最小化和结构风险最小化。

###  经验风险最小化
$$
R_{srm}(f)=\min_{f\in F}\frac{1}{N}\sum_{n=1}^nL(y_i,f(x_i))
$$
当样本容量足够大时，经验风险最小化能够保证有很好的学习效果，在现实中被广泛使用。
>  比如，极大似然估计就是经验风险最小化的例子。当模型是条件概念分布，损失函数是对数缺失函数时，经验最小化就等价于极大似然估计。

当然，当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生后面将叙述的“过拟合”现象。


###  结构风险最小化
$$
\min_{f\in \digamma}\frac{1}{N}\sum_{n=1}^nL(y_i,f(x_i))+\lambda J(f)
$$

结构风险的定义：
$$
R_{srm}(f)=\frac{1}{N}\sum_{n=1}^nL(y_i,f(x_i))+\lambda J(f)
$$
其中J(f)为模型的复杂度，是定义在假设空间$\digamma$上的泛函。模型越复杂，复杂度J(f)就大；反之，模型f越简单，复杂度J(f)就越小。

>  贝叶斯估计中的最大后验概率估计就是结构风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。

结构风险最小化是为了防止过拟合而提出来的策略。结构风险最小化等价于正则化。结构风险在经验风险上加上表示模型复杂度的正则化项或罚项。

----------

##  模型评估与模型选择
###  训练误差与测试误差
统计学习的目的是使学到的模型不仅对已知数据而且对未知数据都能有很好的预测能力。不同的学习方法会给出不同的模型。当损失函数给定时，基于缺失函数的模型的训练误差和模型的测试误就自然成为学习方法评估的标准。注意，统计学习方法具体采用的损失函数未必是评估时使用的损失函数。

假设学习到的模型是$Y=f^{\hat{}}(X)$
训练误差是模型$Y=f^{\wedge}(X)$关于训练数据集的平均损失：
$$
e_{emp}=\frac{1}{N^{'}}I(y_i,f(x_i))
$$
这里I是指示函数，即$Y\neq f(x)$时为1，否则为0.
相应的，常见的测试数据集上的准确为：
$$
r_{test}=\frac{1}{N^{'}}\sum_{i=1}^{N'}I(y_i,f(x_i))
$$
显然，
$$
r_{test} + e_{test} = 1
$$
