---
title: EM算法
date: 2016-11-27 09:03:04
tags:
---


###  最大熵原理
我们的预测应当满足全部已知条件，而对未知的情况不要做任何主观假设。也可以表述为在满足约束条件的模型集体合选取熵最大的模型。


假设现在需要做一个自动将英语到法语的翻译模型，为了方便说明，我们将这个问题简化为将英文句子中的单词{in}翻译成法语词汇。那么翻译模型p就是对于给定包含单词”in”的英文句子，需要给出选择某个法语单词f 做为”in”的翻译结果的概率p(f)。为了帮助开发这个模型，需要收集大量已经翻译好的样本数据。收集好样本之后，接下来需要做两件事情：一是从样本中抽取规则（特征），二是基于这些规则建立模型。
从样本中我们能得到的第一个规则就是in可能被翻译成的法语词汇有：
{dans, en, à, au cours de, pendant}。
也就是说，我们可以给模型p施加第一个约束条件：
p(dans)+p(en)+ p(à)+p(au cours de)+p(pendant) = 1。
这个等式是翻译模型可以用到的第一个对样本的统计信息。显然，有无数可以满足上面约束的模型p可供选择，例如：
p(dans)=1，即这个模型总是预测dans
或者
p(pendant)=1/2 and p(à)=1/2，即模型要么选择预测pendant，要么预测à。
这两个模型都只是在没有足够经验数据的情况下，做的大胆假设。事实上我们只知道当前可能的选项是5个法语词汇，没法确定究竟哪个概率分布式正确。那么，一个更合理的模型假设可能是：
p(dans) = 1/5
p(en) = 1/5
p(à) = 1/5
p(au cours de) = 1/5
p(pendant) = 1/5
即该模型将概率均等地分给5个词汇。但现实情况下，肯定不会这么简单，所以我们尝试收集更多的经验知识。假设我们从语料中发现有30%的情况下，in会被翻译成dans 或者en，那么运用这个知识来更新我们的模型，得到2模型约束：
p(dans) + p(en) = 3/10
p(dans)+p(en)+ p(à)+p(au cours de)+p(pendant) = 1
同样，还是有很多概率分布满足这两个约束。在没有其他知识的情况下，最直观的模型p应该是最均匀的模型（例如，我拿出一个色子问你丢出5的概率是多少，你肯定会回答1/6），也就是在满足约束条件的情况下，将概率均等分配：
p(dans) = 3/20
p(en) = 3/20
p(à) = 7/30
p(au cours de) = 7/30
p(pendant) = 7/30
假设我们再一次观察样本数据，发现：有一半的情况，in被翻译成了dans 或 à。这样，我们有就了3个模型约束：
p(dans) + p(en) = 3/10
p(dans)+p(en)+ p(à)+p(au cours de)+p(pendant) = 1
p(dans)+ p(à)=1/2
我们可以再一次选择满足3个约束的最均匀的模型p，但这一次结果没有那么明显。由于经验知识的增加，问题的复杂度也增加了，归结起来，我们要解决两组问题：第一，均匀(uniform)究竟是什么意思?我们怎样度量一个模型的均匀度(uniformity)？第二，有了上述两个问题的答案，我们如何找到满足所有约束并且均匀的模型？

最大熵算法可以回答上面的2组问题。直观上来将，很简单，即：对已知的知识建模，对未知的知识不做任何假设。换句话说，在给定一组事实（features + output）的情况下，选择符合所有事实，且在其他方面尽可能均匀的模型。这也是我们在上面的例子中，每次选择最恰当的模型用到的原理。俗话说，不把鸡蛋放在一个篮子里，正是运用的这个原理来规避风险。



###  最大熵模型（最大熵分类）
将最大熵原理应用到分类中则为最大熵模型。


在概率论和统计学中，期望值（或数学期望、或均值，亦简称期望，物理学中称为期待值）是指在一个离散性随机变量试验中每次可能结果的概率乘以其结果的总和。
如果两个随机变量的分布相同，则它们的期望值也相同。

在概率论中，任何随机变量的特征函数（缩写：ch.f，复数形式：ch.f's）完全定义了它的概率分布。